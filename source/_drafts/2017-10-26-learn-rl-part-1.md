---
title: 学习强化学习（一）
date: 2017-10-26 09:33:02
tags: reinforcement-learning rl ai deep-learning alpha-go machine-learning ml
---
强化学习最近又火了一把！听说AlphaGo Zero在完全不依赖于人类知识的情况下，自学成才，在150局围棋比赛中，零封已经算是超越人类的高手AlphaGo，人工智能取代人类的日志真的越来越近了？

且不说人工智能是否会取代人类，作为程序员，难道你就不想了解下AlphaGo Zero背后的秘密吗？这一系列博客讲逐步为你揭开它神秘的面纱。

## AlphaGo的神秘武器 —— 强化学习(Reinforcement Learning)
是的是的，你可能已经从新闻中看过这个词了，不过从我自己的感受来看，这个名字取得非常糟糕（相比起来深度学习好像还好理解一点）。强化是一个动词，强化的是东西是什么呢？难道是学习？学习怎么强化啊？强化的是学习本身的方法么？怎么都感觉说不太通。不过没关系，我们慢慢来。

## 来玩游戏吧！
在正式定义和讨论强化学习前，我们先来玩个游戏吧！

### 一个无聊的小“游戏”
由于读者的背景和游戏经验不同，为了尽量避免某种游戏对某些有经验的读者过于简单、对没有经验的读者又过于复杂，这里我定义了一个简单而且纯粹的游戏。你可以在[这个页面](http://voidmain.guru/WebRLGames/dummy-rl-game.html)玩到。

游戏的目标很简单，就是在游戏中得到尽量高的分数。这个“游戏”中你的手柄只有2个按钮，左走还是右走。没做出一个动作，系统都会告诉你当前所处的状态（只有2个状态，左边和右边），同时会给你一定的奖励（也有可能是惩罚）。你要做的就是在找到一个行动策略，能让你在10次行动中得分最高。

我强烈推荐读者先花1分钟玩一下这个页面游戏：[http://voidmain.guru/WebRLGames/dummy-rl-game.html](http://voidmain.guru/WebRLGames/dummy-rl-game.html)

### 最优策略
其实通过几次简单的尝试（毕竟一共就4种组合），你就能发现，这个游戏的最优行动策略是：如果你在左边，向右走（会损失50分），这样你就到了右边，如果你在右边，就向左走（你会得到100分），这样你就回到了左边；然后依次往复，分数会以每次50分，10次行动总分就是500分。

### 我们是怎么做的？
好吧，现在简单回想一下我们的思维方式。当我们面对一个全新规则的游戏，而且我们也不知道游戏内部的原理（内部的状态转移方式），我们是怎么玩的呢？首先我们进行尝试。我们随便挑选一个行为方式，比如，我在左边，就往左边走。这时发现我们一次行动可以获得10分，这不错！

其实我们本可以救这么一直向左走下去，并且以100分结束游戏，这其实也不错。但是我们没有满足，我们会尝试新的走法，例如，例如当前在左边的时候，会尝试往右走。这个时候会发现，我们被惩罚了50分。这（比起之前的+10分）非常糟糕，但是却把我们带到了右边的状态。

在右边的状态，我们同样可以尝试往左和往右，几次尝试以后就能发现最优的组合了。

其实这正是我们每天都在做的事情。我们每天都在做动作（比如你正在阅读我这篇文章来学习强化学习），有的动作可能眼前看起来就有不错的效果，你可以坚持下去；当然也可能有的动作最开始看起来效果不佳，而且代价惨痛（例如你正在浪费你的游戏时间，读我这篇文章，这段时间本来可以打两盘亡者农药？），但是它可能会在日后带给你更多回报（只是可能）。

其中我们最常用的学习方式就是试错。我们从小就学习怎么做题，做对了有奖励，做错了我们就学习为什么做错，整理、分析，以便后续可以做对（获得更高奖励）。

这些其实都是我们非常熟悉、而且自然的东西，只不过现在我们要通过一种方式来教会计算机进行这种试错的学习方式，这就是强化学习要做的事情。


## 正式介绍下强化学习
强化学习是一种迭代式的学习方式。它的学习目标是最终回报最大。为了更明确定义这一目标，我们需要给强化学习要解决的问题提供一些模型。接下来就让我们介绍一些强化学习的一些基本组成元素。

![图1](/assets/learn-rl-1/components.png)
图1. 强化学习的模块及其相互关系

### 环境
说起强化学习，首先必备的一个元素就是环境（Environment）。环境告诉玩家当前所处的“状态”、接受玩家做出的“动作”，并且可以（可能非实时的）告诉用户“行为”的“奖励”。

如果用玩游戏的思维考虑的话，这个环境就是某个游戏。


### 代理
第二个元素的正式称呼是“代理”（Agent，或者叫“特工”），但是由于我比较喜欢以解决游戏问题的角度去思考强化学习，所以我个人喜欢把它称作玩家。

玩家就是参与游戏的人，游戏参与者要做什么呢？最终的表现就是做出一系列的“动作”。例如，你玩PS4游戏的时候，你做出的判断，最终都会体现在对手柄的操作上。是该移动、翻滚还是射击，最终都体现在手柄的操作上。所以玩家的任务就是做出“动作”。

### 状态

对于某些无法完全观测的状态，我们环境返回给我们的是一个“观察”(Observation)，

### 动作
动作是玩家与游戏交互的唯一方式，也是玩家改变游戏状态的唯一方式。

- 奖励 Reward
- 回报 Return
- 策略 Policy
- 值函数
状态值、动作状态值函数
- 目标

## 尝试建模试试
了解了强化学习的组成元素后，我们来思考几个问题，看如何使用强化学习的模型来定义这些问题。

### FlappyBird
FlappyBird当初以困难著称，如果我们想训练强化学习机器人来玩这款游戏，应该怎么将这款游戏抽象出来呢？



### 炉石传说
炉石传说是暴雪推出的一款卡牌游戏，与FlappyBird这种游戏不同，这个游戏充满了随机性（摸牌的顺序是随机的、有些法术的效果也是随机的）与不可观测性（你不知道对手的手牌）。先不考虑这些问题，如果我们想给这个游戏建立一个强化学习模型，该怎么做？

### 开车
一起来说说最近非常火的无人车。假设我们要给自动驾驶建立一个（简单的）强学学习模型，上面各个元素分别对应什么呢？

### 探索迷宫
走迷宫是一个经典的问题。但是它代表着一系列跟时间有关的问题，因为我们希望能尽快走出迷宫或找到宝藏。如果我们的目标中与时间有关，那又应该如何建模呢？

## 挑战
在真正思考解决问题的方法前，我想啰嗦一下，我们来考虑一下，为什么强化学习要解决的问题挺难的。

- 奖励的原因是啥？
- 探索与利用
- 部分观测
- 随机策略才是最好的策略

## GPI - Generalized Policy Iteration
前面说到，

![GPI](/assets/learn-rl-1/gpi.png)

## 剧透一点
那么现在人们是怎么解决问题呢？目前主要有以下几大类方法。

### 有监督学习
其实强化学习也可以用有监督学习的方法来解决。因为在很多的环境中，人类表现的已经很好了。例如开车。假设我们想创造一个强化学习机器人，来学习开车，那么最朴素的想法可能就是跟人学习，而且是跟经验丰富的老司机学习。

这其实就是有监督学习了。当机器人在环境中，处于某个状态，不知道如何处理时，就来“请教”人类专家，而专家则根据自己的经验，来指导机器人做出动作。同时机器人记录这次学习经验，再次经历同样状态时，就可以“正确”做出动作，得到不错的收益。

这种学习方法称为“模仿学习”。顾名思义，就是模仿人类专家进行学习。

但是这种学习方式有2个比较直观的缺陷，首先是这种学习方法需要“人”介入，这就限制了学习的速度，因为人在一段时间内能指导的状态次数毕竟是有限的；第二点就是这种学习方法得到的机器人，表现一般比较难以超过人类。因为它遇到没有经历过的状态，总是倾向于听专家的意见。这就一定程度上制约了算法的极限（永远无法出现AlphaGo Zero了）。

### 直接优化策略
前面我们介绍过了，玩家做出的动作是由“策略”决定的。那么我们是不是可以直接从学习策略入手呢？这就引入了第二类学习方法，叫策略梯度算法（Policy Gradient）。

这种方法的思路也很直观，玩家先随机生成一个策略，然后利用这个策略去玩游戏。经过几次游戏后，玩家就能总结出，这几次游戏最终的收益如何。通过动作和收益，我们就能指导策略进行优化：如果在某个状态S，每次做A动作，最后得到的总回报都比较高，那么我们就应该尽量多的在状态S中做出A动作，即鼓励我们认为回报高的动作尽量多的发生。根据这个原则，在每尝试几次后，就更新一次策略，迭代的优化策略，最终期待策略收敛到最优策略上。

这种迭代的思路就非常类似梯度下降求最优解的方式。我们通过回报计算出策略的“梯度”，然后沿着梯度进行下降（或上升），最终找到最优策略。

现在越来越多的研究人员把策略梯度类算法当作默认算法来解决强化学习问题。其中主要原因是这类算法对问题背景知识要求较少。我可能只需要根据问题设计好我的“策略网络”，剩下的细节就不需要关心了。只要能定义好策略网络，能进行采样，并计算出策略梯度，就可以通过迭代的算法找到最优策略。此外，我们以后会看到策略梯度算法可以非常好的适应动作空间连续的问题。

### 评估回报并优化策略


### 基于模型的学习
对于某些问题，

## 强化学习与深度学习

## 人人都能进行强化学习
与传统的机器学习不同，

## 结论
这篇博客我们讨论了强化学习的定义以及一些可能的解决问题的方法。这都只是一些最基本的思路和想法，我们需要在这之上补充、完善一些细节（涉及到很多数学公式）。不过重要的是，我们现在初步了解了强化学习！

## 节目预告
这只是这个系列博客的开篇，后续我会从环境搭建到强化学习算法实现进行更深入的介绍，如果你感兴趣的话可以继续保持关注。如果你还有时间的话，最好开始看一看Python 3, numpy以及Keras、TensorFlow等深度学习框架相关的内容了！

*由于这个方向并不是我日常工作的方向，所以我只能通过空余时间整理、撰写博客，所以更新速度无法得到保障，希望大家可以多多包涵。 :)*

学习资源

